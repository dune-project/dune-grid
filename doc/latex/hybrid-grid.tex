\documentclass{article}

\usepackage{amsmath}
\usepackage{bold-extra}
\usepackage{booktabs}
\usepackage{comment}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{SIunits}
\usepackage{xspace}

\lstset{
  language=C++,
  basicstyle=\ttfamily, 
  tabsize=4,
  keywordstyle=\bfseries,
  commentstyle=\itshape,
  emph={implementation_defined,ImplementationDefined}, emphstyle=\itshape,
  emph={[2]\#include,\#define,\#ifdef,\#endif}, emphstyle={[2]\bfseries},
  extendedchars=true,
  escapeinside={/*@}{@*/},
  breaklines=true,
}

\newcommand\GHz{\giga\hertz}
\newcommand\s\second

\newcommand\mc\multicolumn

\newcommand\dune{Dune\xspace}
\newcommand\Dune{Dune\xspace}
\newcommand\grid{{\tt dune-grid}\xspace}
\newcommand\GRid{{\tt Dune-Grid}\xspace}
\newcommand\gridhowto{{\tt dune-grid-howto}\xspace}
\newcommand\fem{{\tt dune-fem}\xspace}

\begin{document}

\title{Threading Support for \GRid}

\author{Jö Fahlke}

\maketitle

\tableofcontents

\section{Introduction}

The obvious way to introduce thread support into \grid is to split the
iteration over the {\tt GridView} in some way, such that $P$ threads can
iterate concurrently over different parts of the grid.  Another possibility is
for each thread to iterate over the whole of the {\tt GridView}, but to do
different things on each entity.  Not every grid supports such concurrent
traversal; a grid must be {\em view-thread safe}.

\section{The View-Thread Safe Property}

A view-thread safe grid has the capability {\tt
  Dune::Capabilities::viewThreadSafe<Grid>::v=true}.  It should support
certain operations concurrently by different threads, here is a possibly
incomplete list:
\begin{itemize}
\item Traversal of the grid with different iterators.  One iterator may be
  used first in one thread an later in another, as long as no two threads try
  to modify the same iterator concurrently.
\item Calls to {\tt communicate()}.
\end{itemize}
Other operations will certainly not be supported concurrently by any grid, or
at least not simply because of the view-thread safe property:
\begin{itemize}
\item Calling {\tt mark()}.  (Maybe it should be supported as long as it isn't
  called concurrently for the same entity?  Are there grids that meaningfully
  store the marks in a {\tt std::vector<bool>}?  That one is certainly not
  thread safe for accesses to neighboring elements.  They same may even hold
  true for a {\tt std::vector<signed char>} on some architectures.)
\item Calling {\tt refine()}.
\item Calling {\tt globalRefine()}.
\item Calling {\tt adapt()}.
\end{itemize}

\section{Threads Iterating over Different Parts}

Splitting the grid traversal between different threads can be done in a number
of ways, each with its own pros and cons.  Let me introduce five different
implementations:
\begin{enumerate}
\item \label{imp:filtered-partitioning} We use some partitioner (e.g.\ METIS)
  to generate a partitioning, and store the information as a vector of
  partition numbers, one entry per mesh element.  We then use a filtering
  GridView whose iterators skip over elements from foreign partitions.

  This has been implemented previously by Robert Klöfkorn in \fem, except the
  he used GridParts instead of GridViews \cite{Kloefkorn2012}.
  \begin{description}
  \item[Pro:] It is possible to create partionings with small surface areas.
  \item[Pro:] It is easy to determine whether an intersection is located at
    the boundary of a thread partition.
  \item[Con:] Each thread must iterate over the whole grid.
  \item[Con:] Needs $O(N)$ storage ($N$ number of elements).
  \end{description}
\item \label{imp:entityseed-partitioning} As before, we use some partitioner
  to generate the partioning.  We then iterate other the who host GridView
  once and store the EntitySeeds for all elements in vectors, one for each
  partition.  To iterate over a certain partition, we iterate over the vector
  of EntitySeeds for that partition.
  \begin{description}
  \item[Pro:] It is possible to create partionings with small surface areas.
  \item[Con:] Without additional information (e.g. keeping the partitioning
    information around as for implementation \ref{imp:filtered-partitioning})
    it is infeasible to determine whether an intersection is located at the
    boundary of a thread partition.
  \item[Pro:] Each thread iterates only over it's own partition.
  \item[Con:] Needs $O(N)$ storage ($N$ number of elements).
  \end{description}
\item \label{imp:iterator-ranges} We split the iteration over the host
  GridView into consecutive ranges, by iterating over it once sequentially and
  storing begin and end iterators for each range.
  \begin{description}
  \item[Con:] The resulting partioning will in general have a large surface
    area.
  \item[Con:] Without additional properties of the grid (e.g. iteration order
    is identical to index set order) it is infeasible to determine whether an
    intersection is located at the boundary of a thread partition.
  \item[Pro:] Each thread iterates only over it's own partition.
  \item[Pro:] Needs $O(P)$ storage ($P$ number of partitions).
  \end{description}
\item \label{imp:filtered-every} We use a filtering GridView whose iterators
  stop only at each $P^\text{th}$ element in iteration order.
  \begin{description}
  \item[Con:] The resulting partioning will in general have a very large
    surface area.
  \item[Con:] Without additional properties of the grid (e.g. iteration order
    is identical to index set order) it is infeasible to determine whether an
    intersection is located at the boundary of a thread partition.
  \item[Con:] Each thread must iterate over the whole grid.
  \item[Pro:] Needs $O(1)$ storage ($O(P)$ when all threads have obtained a
    GridView for their partition.
  \end{description}
\item \label{imp:filtered-every-index} We use a filtering GridView whose
  iterators stop only at each $P^\text{th}$ element as determined the index of
  the element.
  \begin{description}
  \item[Con:] The resulting partioning will in general have a very large
    surface area.
  \item[Pro:] It is easy to determine whether an intersection is located at
    the boundary of a thread partition.
  \item[Con:] Each thread must iterate over the whole grid.
  \item[Pro:] Needs $O(1)$ storage ($O(P)$ when all threads have obtained a
    GridView for their partition.)
  \end{description}
\end{enumerate}

The surface area and the ability to determine whether an intersection lies on
the boundary of a thread partition is important in certain numerical schemes.
For instance, a scheme may be implemented in such a way that it normally only
handles each intersection once and ignores it when it encounters it from the
other side.  These schemes typically update the vector entries for both the
inside and the outside element in one go.  This can save computation time
because when the vector entries for neighboring elements are updated in
separate encounters of the intersection certain things have to be calculated
twice.

In a thread parallel context this optimization poses the problem that
different threads might try to update the same vector entries at the same
time.  One could solve this with locking, but it is probably cheaper to simply
avoid the above optimization on thread partition boundaries.  Either way there
is a penalty at thread partition boundaries, so they should be kept small for
such schemes.

\subsection{Codimension Support}

We'll start out with support only for codimension $0$.  This is already
sufficient for many things.  Iteration over entities of other codimensions
isn't usually used during assembly, since it isn't easy to find the other
adjacent entities.  To really do other codimensions correctly, I need some
applications where it is useful to split the traversal of a higher codimension
among multiple threads.  In particular, whether there is any relation needed
between the partitioning for different codimensions.

\subsection{The Interface}

When building GridView adapters that iterate only over parts of the GridView
there are some things that need clarification.  Let us consider the GridView
members one by one.
\begin{description}
\item[constants {\tt conforming}, {\tt dimension}, and {\tt codimension}, type
  {\tt ctype}] These can just be copied from the host GridView.
\item[types {\tt GridViewImp} and {\tt Traits}] Why are these part of the
  interface anyway?  {\tt Traits} may seem useful on first glance, but it is
  actually just used by the {\tt Dune::GridView} facade to extract all the
  types.
\item[type {\tt Grid}, function \tt grid()] This should be the same in the
  adapter as in the host.
\item[type {\tt IndexSet}, function \tt indexSet()] It is probably most useful
  to just return the same here as in the host GridView.  Typically, we want to
  use the index set to access vector entries.  During multi-threaded assembly
  we still want to use the host GridView's index set to access the vector.
  This way we can simply reuse assembling code.

  The alternative would be to build a new index set covering only entities in
  the thread partition.  This is however difficult to do an requires $O(N)$
  storage in general.

  A third alternative would be simply to not implement this method and type.
\item[types {\tt Intersection} and {\tt IntersectionIterator}, functions {\tt
    ibegin()} and {\tt iend()}] These should be identical to the host
  GridView.
\item[type {\tt CollectiveCommunication}, function {\tt comm()}] Communication
  inside a concurrent grid traversal makes little sense.  In particular, we
  haven't even touched the issue of thread safety of \dune's collective
  communication.  However, there may be contexts other than multi-threading
  where this kind of partitioning inside one process can be useful.  In any
  case it shouldn't hurt for the adapted GridView to provide the same
  collective communication as the host GridView.
\item[functions {\tt size(int codim)} and {\tt size(GeometryType)}] These are
  tricky.  \Dune's standard GridView just forwards them to the index set.  We
  should probably do the same, even if the index set actually comes from the
  host GridView.  The reason is that is has little connection to the concept
  of the GridView: it allows to count the number of entities of a certain
  GeometryType, but GridViews don't allow to iterate over just those elements.
  It allows to count the number of entities in a certain codimension, but only
  for the all partition, we have not way to count just the elements e.g. in
  the interior partition, except by iterating over them.
\item[function {\tt contains()}] This should only return true if this entity
  can be encountered by iterating over this adapted GridView.  This provides
  one simple way to check whether e.g.\ a neighboring entity belongs to the
  same thread partition.  Of course, this semantic makes it impossible to
  provide this function in some of the implementations above.
\item[functions {\tt begin<codim>()}, {\tt begin<codim, pitype>()}, {\tt
    end<codim>()} and {\tt end<codim, pitype>()}] These should return
  iterators of the given partition iterator type that only encounter entities
  of the given thread partition.  The type of the iterator will generally
  differ the the iterator type of the host GridView.
\item[struct {\tt Codim}] Essentially identical to the host GridView, except
  the types of the iterators will be different.
\end{description}

\section{First Results}

\begin{table}
  \centering
  \begin{tabular}{lcccc}
    \toprule
    Grid Manager & \mc2c{Mean Runtime [\s]}        & Speedup & Efficiency [\%] \\
           \cmidrule(lr){2-3}
         & Sequential & C++-Threads ($2\times$) &         &                 \\
    \midrule
% BEGIN RECEIVE ORGTBL tbl:threading-first-results
ALUSimplex & 44.6101 & 32.5721 & 1.37 & 68 \\
Yasp & 27.9924 & 16.3608 & 1.71 & 86 \\
% END RECEIVE ORGTBL tbl:threading-first-results
    \bottomrule
  \end{tabular}
  \caption{Speedup for the modified {\tt finitevolume.cc} from \gridhowto.
    Both grid managers were used on a unit square, generated with the {\tt
      StructuredGridFactory}, with {\tt globalRefine(8)} applied for YaspGrid
    and {\tt globalRefine(7)} applied for ALUSimplexGrid.  The code
    did not used the one-sided assembly on intersections-optimization; that
    actually turned out not to make much of a difference for this example.
    The version parallelized with C++-threads used two threads, the same as the
    number of physical cores on the test system (Intel® Core™2 Duo CPU P9400
    @2.40\GHz).  The timings are mean values of four runs.}
  \label{tbl:threading-first-results}
\end{table}
\begin{comment}
#+ORGTBL: SEND tbl:threading-first-results orgtbl-to-latex :splice t :skip 2
| Grid       | Sequential | OpenMP(2) | Speedup | Efficiency [%] |
|------------+------------+-----------+---------+----------------|
| ALUSimplex |    44.6101 |   32.5721 |    1.37 |             68 |
| Yasp       |    27.9924 |   16.3608 |    1.71 |             86 |
#+TBLFM: $4=$-2/$-1;%.2f::$5=100*$-3/$-2/2;%.0f
The above table is an extract of the table below, showing only the mean values
for the Plain case.

# Raw data:
# - ALUGrid          eea71a0493cc8fb441d6c23be05dea8530ea58f4
#                    (1.52 tarball with fixes by Andi Buhr)
# - dune-common      181a58430ff92dd6f060bf29b0d025571cb8024d
#   (892d2043ab7554c69e6c99b985a33ced4d3bc627 +patch to disable
#    CHECK_INTERFACE_IMPLEMENTATION,
#    http://users.dune-project.org/repositories/projects/hybrid-grid-dune-common.git)
# - dune-geometry    46256e328b40438dd27630464d9167aeb2b4a779
# - dune-grid        9f59b23f1825acaf3cc9a2fa3936220ff447534c
#   (86182d33be54bfcce5d121b34fb6cb0edabf78a2 +updated .gitignores,
#    45a0c58b64931d61986ac14ee044bbd502d53b55 +disabled FiniteStack,
#    http://users.dune-project.org/repositories/projects/hybrid-grid-dune-grid.git)
# - dune-hybrid-grid 6f1f5ae752a33313c02845d386be05f41c5a4274
#   (http://users.dune-project.org/repositories/projects/hybrid-grid-dune-hybrid-grid.git)
# - g++ (Debian 4.7.2-5) 4.7.2
# - optimization options: -O3 -funroll-loops -fno-strict-aliasing
# - Opt means one sided assembly on intersections, where possible, Plain means
#   twosided.
# - CPU was "Intel(R) Core(TM)2 Duo CPU     P9400  @ 2.40GHz"
# - Grid was the unit cube from the StructuredGridFactory, with
#   globalRefine(7) applied for ALUGrid and globalRefine(8) for YaspGrid and
#   SGrid.
|            |   Plain |   Plain | Plain |     Opt |     Opt |   Opt |
| Grid       |     Seq | OSTh(2) | E [%] |     Seq | OSTh(2) | E [%] |
|------------+---------+---------+-------+---------+---------+-------|
| ALUSimplex | 45.0167 | 32.0758 |       | 45.4015 | 32.7205 |       |
| ref=7      | 44.6010 | 33.1822 |       | 45.2705 | 33.2635 |       |
|            | 44.5574 | 33.6564 |       | 45.1447 | 34.1416 |       |
|            | 44.2652 | 31.3739 |       | 45.0224 | 33.1322 |       |
|------------+---------+---------+-------+---------+---------+-------|
| mean       | 44.6101 | 32.5721 |    68 | 45.2098 | 33.3145 |    68 |
| stddev     |  0.3094 |  1.0376 |       |  0.1631 |  0.5980 |       |
|------------+---------+---------+-------+---------+---------+-------|
| Yasp       | 27.7778 | 19.1363 |       | 30.4038 | 16.9045 |       |
| ref=8      | 28.2098 | 15.3266 |       | 34.0757 | 16.4706 |       |
|            | 28.2842 | 15.4153 |       | 30.4185 | 16.7239 |       |
|            | 27.6977 | 15.5652 |       | 29.8972 | 16.8037 |       |
|------------+---------+---------+-------+---------+---------+-------|
| mean       | 27.9924 | 16.3608 |    86 | 31.1988 | 16.7257 |    93 |
| stddev     |  0.2974 |  1.8529 |       |  1.9332 |  0.1854 |       |
|------------+---------+---------+-------+---------+---------+-------|
| SGrid      | 33.3188 |         |       | 38.4664 |         |       |
| ref=8      | 32.1779 |         |       | 38.8568 |         |       |
|            | 33.7675 |         |       | 38.6083 |         |       |
|            | 34.5424 |         |       | 39.3417 |         |       |
|------------+---------+---------+-------+---------+---------+-------|
| mean       | 33.4517 |         |       | 38.8183 |         |       |
| stddev     |  0.9882 |         |       |  0.3844 |         |       |
#+TBLFM: @7$2=vmean(@-II..@-I);%.4f::@7$3=vmean(@-II..@-I);%.4f::@7$4=100*($-2/$-1)/2;%.0f::@7$5=vmean(@-II..@-I);%.4f::@7$6=vmean(@-II..@-I);%.4f::@7$7=100*($-2/$-1)/2;%.0f::@8$2=vsdev(@-II..@-I);%.4f::@8$3=vsdev(@-II..@-I);%.4f::@8$5=vsdev(@-II..@-I);%.4f::@8$6=vsdev(@-II..@-I);%.4f::@13$2=vmean(@-II..@-I);%.4f::@13$3=vmean(@-II..@-I);%.4f::@13$4=100*($-2/$-1)/2;%.0f::@13$5=vmean(@-II..@-I);%.4f::@13$6=vmean(@-II..@-I);%.4f::@13$7=100*($-2/$-1)/2;%.0f::@14$2=vsdev(@-II..@-I);%.4f::@14$3=vsdev(@-II..@-I);%.4f::@14$5=vsdev(@-II..@-I);%.4f::@14$6=vsdev(@-II..@-I);%.4f::@19$2=vmean(@-II..@-I);%.4f::@19$5=vmean(@-II..@-I);%.4f::@20$2=vsdev(@-II..@-I);%.4f::@20$5=vsdev(@-II..@-I);%.4f

\end{comment}

I did an implementation of \ref{imp:filtered-every-index} (taking every
$P^\text{th}$ entity based on the index set) and used the {\tt
  finitevolume.cc} example from \gridhowto to do a little benchmarking.  There
were some modifications necessary: complete removal of the ``assemble only
once on intersections'' optimization,\footnote{I also tried timings with this
  optimization, but this did not make such a big difference for this example.
  I would however expect that this optimization becomes important for
  higher-order or other methods where there is a lot of work on each element.}
using the structured gridfactory to generate 2D meshes, making it possible to
compile programs for different grids, and some restructuring of the code to
allow for better reuse.  For multithread-support I used C++ threads.  I also
wrote some unit tests to ensure that sequential and threaded version produce
identical results.\footnote{This worked unexpectedly well -- the output is in
  fact bit-identical, so I don't even have to employ {\tt fuzzy-diff}.}
Compilation was done using g++ (Debian 4.7.2-5) with optimization {\tt -O3
  -funroll-loops -fno-strict-aliasing}.

The timings are in table \ref{tbl:threading-first-results}.  The efficiency is
decent with around 70\% for ALUSimplexGrid and around 85\% for YaspGrid,
considering finite volume actually does very little work per element.
ALUSimplex needed some modifications in the memory management of its
\dune-wrapper to make it thread safe, basically disabling it, which resulted
in a substantial slow down.  Robert Klöfkorn apparently has a better solution
for the next release of ALUGrid, which will be available
soonish.\footnote{This statement was brought to you 2013-07-30} SGrid was also
tried, but turned out not to be thread safe, and not attempt was made to make
it thread safe.

\bibliographystyle{plain}
\bibliography{hybrid-grid}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% mode: TeX-PDF
%%% mode: orgtbl
%%% TeX-master: t
%%% mode: flyspell
%%% ispell-local-dictionary: "american"
%%% End:
